# This script is used to train the model
# Juexiao
# Data: 2019-09-19

#!/usr/bin/env python3

import numpy as np
import tensorflow as tf
import sys, os
import argparse
from sklearn import metrics
from model import Net

############ Parameters ############
BATCH_SIZE = 64
PATCH_SIZE = 10
DEPTH = 32
NUM_HIDDEN = 64
SEQ_LEN = 176 + 2*PATCH_SIZE-2
NUM_CHANNELS = 4
NUM_LABELS = 2
NUM_EPOCHS = 100
############ **************** ############

def pad_dataset1(dataset, labels):
	''' Pad sequences to (length + 2*DEPTH - 2) wtih 0.25 '''
	new_dataset = np.ones([dataset.shape[0], dataset.shape[1]+2*PATCH_SIZE-2, dataset.shape[2], dataset.shape[3]], dtype = np.float32) * 0.25
	new_dataset[:, PATCH_SIZE-1:-(PATCH_SIZE-1), :, :] = dataset
	labels = (np.arange(NUM_LABELS) == labels[:,None]).astype(np.float32)
	return new_dataset, labels

def pad_dataset2(dataset, labels):
	''' Pad sequences to (length + 2*DEPTH - 2) wtih mean(coverage) '''
	new_dataset = np.ones([dataset.shape[0], dataset.shape[1]+2*PATCH_SIZE-2, dataset.shape[2], dataset.shape[3]], dtype = np.float32) * np.mean(dataset)
	new_dataset[:, PATCH_SIZE-1:-(PATCH_SIZE-1), :, :] = dataset
	labels = (np.arange(NUM_LABELS) == labels[:,None]).astype(np.float32)
	return new_dataset, labels

def get_accuracy(predictions, labels):
	  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))
		  / predictions.shape[0])

def get_pref(predictions, labels):
	preds=np.argmax(predictions, 1)
	labels=np.argmax(labels, 1)
	TP = 0
	FP = 0
	TN = 0
	FN = 0
	for i in range(len(preds)):
		if labels[i]==preds[i]==0:
				TP += 1
		if preds[i]==1 and labels[i]!=preds[i]:
				FN += 1
		if labels[i]==preds[i]==1:
				TN += 1
		if preds[i]==0 and labels[i]!=preds[i]:
				FP += 1
	return(TP, FP, TN, FN)

def perf_measure(y_actual, y_hat, ID):
	TP = 0
	FP = 0
	TN = 0
	FN = 0
	SET=[]

	for i in range(len(y_hat)):
		if y_actual[i]==y_hat[i]==0:
				TP += 1
				SET.append((ID[i],'TP'))
		if y_hat[i]==1 and y_actual[i]!=y_hat[i]:
				FN += 1
				SET.append((ID[i],'FN'))
		if y_actual[i]==y_hat[i]==1:
				TN += 1
				SET.append((ID[i],'TN'))
		if y_hat[i]==0 and y_actual[i]!=y_hat[i]:
				FP += 1
				SET.append((ID[i],'FP'))

	return(TP, FP, TN, FN, SET)

if __name__ == '__main__':

   # with tf.device('/cpu:0'):
	parser = argparse.ArgumentParser()
	parser.add_argument('data', help='Path to data npz files')
	parser.add_argument('--out', default=None, help='Save model weights to (.npz file)')
	parser.add_argument('--hparam', default=None, help='Hyper-params (.npz file), default using random hyper-params')
	parser.add_argument('--trainid', default=None, help='Train ID')
	opts = parser.parse_args()
	data = opts.data
	out=opts.out
	hparam=opts.hparam
	TRAIN_ID = opts.trainid


	pretrained=None

		# Load and pad data
	data = np.load(data)
	train_data1, train_labels = pad_dataset1(data['train_dataset1'], data['train_labels'])
	train_data2, train_labels = pad_dataset2(data['train_dataset2'], data['train_labels'])
	valid_data1, valid_labels = pad_dataset1(data['valid_dataset1'], data['valid_labels'])
	valid_data2, valid_labels = pad_dataset2(data['valid_dataset2'], data['valid_labels'])
	valid_data3=data['valid_dataset3']

		# Build model and trainning graph
	hyper_dict = dict(np.load(hparam)) if hparam is not None else None
	model = Net(hyper_dict)
	model.build_training_graph()

		# Building record file
		#with open('out/%s_par.txt'%TRAIN_ID,'w') as w:
		#	w.write('#'+str(model.hyper_dict)+'\n')

	W=open('out/%s.txt'%TRAIN_ID,'w')
	W.write('epoch\ttrain_score\tvalid_score\tTP\tFP\tTN\tFN\terror_rate\taccuracy_score\tmicro_precision_score\tmacro_precision_score\tmicro_recall_score\tmacro_recall_score\tf1_score\tcohen_kappa_score\troc_auc_score\n')


		# Kick off training
	config = tf.ConfigProto(allow_soft_placement=True)
	sess = tf.Session(config=config)
	writer = tf.summary.FileWriter('graphs/', sess.graph)
	sess.run(tf.global_variables_initializer())

	if pretrained is not None:
		model.load_weights(pretrained, sess)
		print('Fine-tuning the pre-trained model %s'%pretrained)
	else:
		print('Initialized')
	pred = model.get_prediction(sess, valid_data1, valid_data2, False)
	tp,fp,tn,fn  = get_pref(pred, valid_labels)
	accuracy = (tp+tn)/(tp+tn+fp+fn)
	precision = tp/(tp+fp)
	recall =  tp/(tp+fn)
	f1score = 2*precision*recall/(precision+recall)
	print('Validation accuracy/precision/f1score at the beginning: %.5f%%, %.5f%%, %.5f%%' %(accuracy,precision,f1score))

	train_resuts, valid_results = [], []
	save_weights = []
	for epoch in range(NUM_EPOCHS):
		permutation = np.random.permutation(train_labels.shape[0])
		shuffled_dataset1 = train_data1[permutation, :, :]
		shuffled_dataset2 = train_data2[permutation, :, :]
		shuffled_labels = train_labels[permutation, :]
		tps = 0.
		fps = 0.
		tns = 0.
		fns = 0.
		for step in range(shuffled_labels.shape[0] // BATCH_SIZE):
			offset = step * BATCH_SIZE
			batch_data1 = train_data1[offset:(offset+BATCH_SIZE), :, :, :]
			batch_data2 = train_data2[offset:(offset+BATCH_SIZE), :, :, :]
			batch_labels = train_labels[offset:(offset+BATCH_SIZE), :]
			fd = {
				model.dataset1: batch_data1,
				model.dataset2: batch_data2,
				model.labels: batch_labels,
				model.istrain: True
			}
			_, pred = sess.run([model.optimizeOp, model.prediction], feed_dict=fd)
			tp,fp,tn,fn  = get_pref(pred, batch_labels)
			tps += tp
			fps += fp
			tns += tn
			fns += fn
			sess.run(model.stepOp)

		accuracy = (tps+tns)/(tps+tns+fps+fns)
		precision = tps/(tps+fps)
		recall = tps/(tps+fns)
		f1score = 2*precision*recall/(precision+recall)
		pred = model.get_prediction(sess, valid_data1, valid_data2, False)
		vtp,vfp,vtn,vfn = get_pref(pred, valid_labels)
		val_accuracy = (vtp+vtn)/(vtp+vtn+vfp+vfn)
		val_precision = vtp/(vtp+vfp)
		val_recall = vtp/(vtp+vfn)
		val_f1score = 2*val_precision*val_recall/(val_precision+val_recall)
		#####append evaluation score
		train_resuts.append(precision)
		valid_results.append(val_precision)

			# Run Evaluation
		preds_new=np.argmax(pred, 1)
		labels_new=np.argmax(valid_labels, 1)

		TP,FP,TN,FN,SET=perf_measure(labels_new,preds_new,valid_data3)
		error_rate=1-(TP+TN)/(TP+TN+FP+FN)
		accuracy_score=metrics.accuracy_score(labels_new,preds_new)
		micro_precision_score=metrics.precision_score(labels_new,preds_new, average='micro')
		macro_precision_score=metrics.precision_score(labels_new,preds_new, average='macro')
		micro_recall_score=metrics.recall_score(labels_new,preds_new, average='micro')
		macro_recall_score=metrics.recall_score(labels_new,preds_new, average='macro')
		f1_score=metrics.f1_score(labels_new,preds_new, average='weighted')
		cohen_kappa_score=metrics.cohen_kappa_score(labels_new,preds_new)
		roc_auc_score=metrics.roc_auc_score(labels_new,preds_new)

		W.write('%s\t%s\t%s\t%s\t%s\t%s\t%s\t%s\t%s\t%s\t%s\t%s\t%s\t%s\t%s\t%s\n'%(str(epoch),str(train_resuts[-1]),\
		str(valid_results[-1]),str(TP),str(FP),\
		str(TN),str(FN),str(error_rate),str(accuracy_score),str(micro_precision_score),str(macro_precision_score),\
		str(micro_recall_score),str(macro_recall_score),str(f1_score),str(cohen_kappa_score),\
		str(roc_auc_score)))

		print('Epoch: %d'%epoch)
		print('Training accuracy/precision/f1score: %.5f%%, %.5f%%, %.5f%%' % (accuracy, precision,f1score))
		print('Validation accuracy/precision/f1score: %.5f%%, %.5f%%, %.5f%%' %(val_accuracy,val_precision,val_f1score))

			# Early stopping based on validation results
		if epoch > 20 and valid_results[-21] > max(valid_results[-20:]):
			train_resuts = train_resuts[:-20]
			valid_results = valid_results[:-20]
			print('\n\n########################\nFinal result:')
			print("Best valid epoch: %d"%(len(train_resuts)-1))
			print("Training measure score: %.5f%%"%train_resuts[-1])
			print("Validation measure score: %.5f%%"%valid_results[-1])

			np.savez('out/%s_ID'%TRAIN_ID,np.array(SET))
			if out is not None:
				np.savez(out, **save_weights[0])
			break

			# Model saving
		sw = sess.run(model.weights)
		save_weights.append(sw)
		if epoch > 20:
			save_weights.pop(0)

		if epoch == NUM_EPOCHS-1:
			print('\n\n########################\nFinal result:')
			print("Best valid epoch: %d"%(len(train_resuts)-1))
			print("Training measure: %.5f%%"%train_resuts[-1])
			print("Validation measure: %.5f%%"%valid_results[-1])

			np.savez('out/%s_ID'%TRAIN_ID,np.array(SET))
			if out is not None:
				np.savez(out, **save_weights[-1])

	W.close()
	writer.close()
	print('TRAIN_ID: %s'%TRAIN_ID)
