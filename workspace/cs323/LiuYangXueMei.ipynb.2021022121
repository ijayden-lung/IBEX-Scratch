{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(3072,33)\n",
    "        self.fc2 = nn.Linear(33,10)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = data\n",
    "        x = x.view(-1, 3 * 32 * 32)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        m = nn.Sigmoid()\n",
    "        x = m(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = MyNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyNet(\n",
      "  (fc1): Linear(in_features=3072, out_features=33, bias=True)\n",
      "  (fc2): Linear(in_features=33, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    #loss_=torch.nn.BCELoss()\n",
    "    loss_ = nn.CrossEntropyLoss()\n",
    "    running_loss = 0.0\n",
    "    for data in trainloader:\n",
    "        #label=data.y\n",
    "        data,label = data\n",
    "        #print(data.shape)\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        #print(output)\n",
    "        #print(label)\n",
    "        output = output.type(torch.FloatTensor)\n",
    "        loss = loss_(output, label)\n",
    "       \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(running_loss)\n",
    "    \n",
    "    return running_loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24420.630351305008\n",
      "Epoch: 001, Loss: 24420.63035\n",
      "24299.381445765495\n",
      "Epoch: 002, Loss: 24299.38145\n",
      "24226.58961868286\n",
      "Epoch: 003, Loss: 24226.58962\n",
      "24191.45047199726\n",
      "Epoch: 004, Loss: 24191.45047\n",
      "24148.75768816471\n",
      "Epoch: 005, Loss: 24148.75769\n",
      "24135.28552341461\n",
      "Epoch: 006, Loss: 24135.28552\n",
      "24140.44077539444\n",
      "Epoch: 007, Loss: 24140.44078\n",
      "24203.630521297455\n",
      "Epoch: 008, Loss: 24203.63052\n",
      "24105.113616466522\n",
      "Epoch: 009, Loss: 24105.11362\n",
      "24183.035018205643\n",
      "Epoch: 010, Loss: 24183.03502\n",
      "24235.299500346184\n",
      "Epoch: 011, Loss: 24235.29950\n",
      "24244.091691613197\n",
      "Epoch: 012, Loss: 24244.09169\n",
      "24234.525854349136\n",
      "Epoch: 013, Loss: 24234.52585\n",
      "24193.356850862503\n",
      "Epoch: 014, Loss: 24193.35685\n",
      "24230.370767831802\n",
      "Epoch: 015, Loss: 24230.37077\n",
      "24287.735668301582\n",
      "Epoch: 016, Loss: 24287.73567\n",
      "24251.82266664505\n",
      "Epoch: 017, Loss: 24251.82267\n",
      "24328.495512366295\n",
      "Epoch: 018, Loss: 24328.49551\n",
      "24287.033247947693\n",
      "Epoch: 019, Loss: 24287.03325\n",
      "24308.328745126724\n",
      "Epoch: 020, Loss: 24308.32875\n",
      "24331.383257985115\n",
      "Epoch: 021, Loss: 24331.38326\n",
      "24314.866762161255\n",
      "Epoch: 022, Loss: 24314.86676\n",
      "24464.223432183266\n",
      "Epoch: 023, Loss: 24464.22343\n",
      "24397.043234944344\n",
      "Epoch: 024, Loss: 24397.04323\n",
      "24467.215878605843\n",
      "Epoch: 025, Loss: 24467.21588\n",
      "24498.367102622986\n",
      "Epoch: 026, Loss: 24498.36710\n",
      "24510.426126122475\n",
      "Epoch: 027, Loss: 24510.42613\n",
      "24485.36825609207\n",
      "Epoch: 028, Loss: 24485.36826\n",
      "24489.164442300797\n",
      "Epoch: 029, Loss: 24489.16444\n",
      "24577.58028793335\n",
      "Epoch: 030, Loss: 24577.58029\n",
      "24567.923547506332\n",
      "Epoch: 031, Loss: 24567.92355\n",
      "24634.636747837067\n",
      "Epoch: 032, Loss: 24634.63675\n",
      "24665.762397527695\n",
      "Epoch: 033, Loss: 24665.76240\n",
      "24732.31202685833\n",
      "Epoch: 034, Loss: 24732.31203\n",
      "24690.28101480007\n",
      "Epoch: 035, Loss: 24690.28101\n",
      "24545.6367944479\n",
      "Epoch: 036, Loss: 24545.63679\n",
      "24720.230105042458\n",
      "Epoch: 037, Loss: 24720.23011\n",
      "24678.441685318947\n",
      "Epoch: 038, Loss: 24678.44169\n",
      "24621.479608654976\n",
      "Epoch: 039, Loss: 24621.47961\n",
      "24677.307659626007\n",
      "Epoch: 040, Loss: 24677.30766\n",
      "24661.00837469101\n",
      "Epoch: 041, Loss: 24661.00837\n",
      "24823.110397577286\n",
      "Epoch: 042, Loss: 24823.11040\n",
      "24800.540077209473\n",
      "Epoch: 043, Loss: 24800.54008\n",
      "24738.966066241264\n",
      "Epoch: 044, Loss: 24738.96607\n",
      "24926.716539382935\n",
      "Epoch: 045, Loss: 24926.71654\n",
      "24971.112972974777\n",
      "Epoch: 046, Loss: 24971.11297\n",
      "24881.752140164375\n",
      "Epoch: 047, Loss: 24881.75214\n",
      "24906.801693320274\n",
      "Epoch: 048, Loss: 24906.80169\n",
      "25075.749056577682\n",
      "Epoch: 049, Loss: 25075.74906\n",
      "25003.544363856316\n",
      "Epoch: 050, Loss: 25003.54436\n",
      "25004.795180678368\n",
      "Epoch: 051, Loss: 25004.79518\n",
      "25040.57374060154\n",
      "Epoch: 052, Loss: 25040.57374\n",
      "25107.536075353622\n",
      "Epoch: 053, Loss: 25107.53608\n",
      "25101.62636733055\n",
      "Epoch: 054, Loss: 25101.62637\n",
      "25010.695837020874\n",
      "Epoch: 055, Loss: 25010.69584\n",
      "25062.78011906147\n",
      "Epoch: 056, Loss: 25062.78012\n",
      "25030.902292609215\n",
      "Epoch: 057, Loss: 25030.90229\n",
      "25134.568180918694\n",
      "Epoch: 058, Loss: 25134.56818\n",
      "25129.863354682922\n",
      "Epoch: 059, Loss: 25129.86335\n",
      "25153.316761255264\n",
      "Epoch: 060, Loss: 25153.31676\n",
      "25120.292207837105\n",
      "Epoch: 061, Loss: 25120.29221\n",
      "25248.48306107521\n",
      "Epoch: 062, Loss: 25248.48306\n",
      "25288.204061746597\n",
      "Epoch: 063, Loss: 25288.20406\n",
      "25218.197964906693\n",
      "Epoch: 064, Loss: 25218.19796\n",
      "25144.257699251175\n",
      "Epoch: 065, Loss: 25144.25770\n",
      "25203.988067626953\n",
      "Epoch: 066, Loss: 25203.98807\n",
      "25231.488210439682\n",
      "Epoch: 067, Loss: 25231.48821\n",
      "25319.993853211403\n",
      "Epoch: 068, Loss: 25319.99385\n",
      "25434.126246333122\n",
      "Epoch: 069, Loss: 25434.12625\n",
      "25442.425097107887\n",
      "Epoch: 070, Loss: 25442.42510\n",
      "25512.846695899963\n",
      "Epoch: 071, Loss: 25512.84670\n",
      "25446.464229226112\n",
      "Epoch: 072, Loss: 25446.46423\n",
      "25475.96031999588\n",
      "Epoch: 073, Loss: 25475.96032\n",
      "25517.916803240776\n",
      "Epoch: 074, Loss: 25517.91680\n",
      "25313.173617243767\n",
      "Epoch: 075, Loss: 25313.17362\n",
      "25547.937327861786\n",
      "Epoch: 076, Loss: 25547.93733\n",
      "25676.867297410965\n",
      "Epoch: 077, Loss: 25676.86730\n",
      "25456.584562301636\n",
      "Epoch: 078, Loss: 25456.58456\n",
      "25383.25031721592\n",
      "Epoch: 079, Loss: 25383.25032\n",
      "25469.88028872013\n",
      "Epoch: 080, Loss: 25469.88029\n",
      "25601.767033100128\n",
      "Epoch: 081, Loss: 25601.76703\n",
      "25637.386016488075\n",
      "Epoch: 082, Loss: 25637.38602\n",
      "25722.46411061287\n",
      "Epoch: 083, Loss: 25722.46411\n",
      "25669.68518960476\n",
      "Epoch: 084, Loss: 25669.68519\n",
      "25768.913040995598\n",
      "Epoch: 085, Loss: 25768.91304\n",
      "25651.897927761078\n",
      "Epoch: 086, Loss: 25651.89793\n",
      "25627.746871352196\n",
      "Epoch: 087, Loss: 25627.74687\n",
      "25701.223291397095\n",
      "Epoch: 088, Loss: 25701.22329\n",
      "25846.354849100113\n",
      "Epoch: 089, Loss: 25846.35485\n",
      "25934.544064998627\n",
      "Epoch: 090, Loss: 25934.54406\n",
      "25762.49344432354\n",
      "Epoch: 091, Loss: 25762.49344\n",
      "25687.143394827843\n",
      "Epoch: 092, Loss: 25687.14339\n",
      "25534.75672698021\n",
      "Epoch: 093, Loss: 25534.75673\n",
      "25585.566499114037\n",
      "Epoch: 094, Loss: 25585.56650\n",
      "25683.097769618034\n",
      "Epoch: 095, Loss: 25683.09777\n",
      "25652.355556368828\n",
      "Epoch: 096, Loss: 25652.35556\n",
      "25692.49768769741\n",
      "Epoch: 097, Loss: 25692.49769\n",
      "25657.39044392109\n",
      "Epoch: 098, Loss: 25657.39044\n",
      "25618.868763446808\n",
      "Epoch: 099, Loss: 25618.86876\n",
      "25537.92044377327\n",
      "Epoch: 100, Loss: 25537.92044\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 101):\n",
    "    loss = train()\n",
    "    print('Epoch: {:03d}, Loss: {:.5f}'.\n",
    "          format(epoch, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
