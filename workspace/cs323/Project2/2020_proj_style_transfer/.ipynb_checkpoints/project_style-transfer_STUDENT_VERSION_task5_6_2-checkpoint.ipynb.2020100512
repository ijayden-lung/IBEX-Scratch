{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LONG Yongkang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all needed resources\n",
    "%matplotlib inline\n",
    "\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import os\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "import requests\n",
    "from torchvision import transforms, models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tesla V100-SXM2-32GB'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(img_path, max_size = 400, shape = None):\n",
    "    \n",
    "    ''' Load and downscale an image if the shorter side is longer than <max_size> px '''\n",
    "    \n",
    "    if \"http\" in img_path:\n",
    "        response = requests.get(img_path)\n",
    "        image = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "    else:\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "    \n",
    "    if min(image.size) > max_size:\n",
    "        size = max_size\n",
    "    else:\n",
    "        size = min(image.size)\n",
    "    \n",
    "    if shape is not None:\n",
    "        size = shape\n",
    "    \n",
    "    in_transform = transforms.Compose([\n",
    "                        transforms.Resize(size),\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize((0.485, 0.456, 0.406), \n",
    "                                             (0.229, 0.224, 0.225))])\n",
    "\n",
    "    # discard alpha channel (:3) and append the batch dimension (unsqueeze(0))\n",
    "    image = in_transform(image)[:3,:,:].unsqueeze(0)\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for un-normalizing an image \n",
    "# and converting it from a Tensor image to a NumPy image for display\n",
    "def im_convert(tensor):\n",
    "    \"\"\" Convert a PyTorch tensor to a NumPy image. \"\"\"\n",
    "    \n",
    "    image = tensor.to(\"cpu\").clone().detach()\n",
    "    image = image.numpy().squeeze()\n",
    "    image = image.transpose(1,2,0)\n",
    "    image = image * np.array((0.229, 0.224, 0.225)) + np.array((0.485, 0.456, 0.406))\n",
    "    image = image.clip(0, 1)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(image, model, layers):\n",
    "    # Run an image forward through a model and get the features for a set of layers.\n",
    "        \n",
    "    features = {}\n",
    "    x = image\n",
    "    # model._modules is a dictionary holding each module in the model\n",
    "    for name, layer in model._modules.items():\n",
    "        x = layer(x)\n",
    "        if name in layers:\n",
    "            features[layers[name]] = x\n",
    "            \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def result_to_hd(image, filePath):\n",
    "    image = im_convert(image)\n",
    "\n",
    "    formatted = (image * 255 / np.max(image)).astype('uint8')\n",
    "    pil_image = Image.fromarray(formatted, 'RGB')\n",
    "    pil_image.save(filePath, \"JPEG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptual Losses for Real-Time Style Transfer and Super-Resolution [5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of solving an optimization problem for each target image, Johnson et al. [5] train a feed forward neural network for style transfer. Your task now is to re-implement certain aspects of their method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions and classes, you do not have to change anything here, you can however if these functions do not fully satisfy your needs\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        self.total_imgs = os.listdir(root)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.total_imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return load_image(os.path.join(self.root, self.total_imgs[idx]))[0]\n",
    "    \n",
    "def get_checkpoint_dir(suffix = None):\n",
    "    max_num = 0\n",
    "\n",
    "    dirs = [os.path.basename(x[0]) for x in os.walk('checkpoints')]\n",
    "\n",
    "    for dir in dirs:\n",
    "        elems = dir.split('_')\n",
    "        if len(elems) > 0:\n",
    "            prefix = elems[0]\n",
    "\n",
    "            if len(prefix) > 0 and all(map(str.isdigit, prefix)):\n",
    "                num = int(prefix)\n",
    "\n",
    "                if max_num <= num:\n",
    "                    max_num = num + 1\n",
    "\n",
    "    checkpoint_dir = os.path.join('checkpoints', str(max_num).zfill(5) + '_checkpoint_' + (suffix if not suffix == None else ''))\n",
    "\n",
    "    return max_num, checkpoint_dir\n",
    "\n",
    "def write_checkpoint(checkpoint_dir, model, optimizer, iter_number):\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "\n",
    "    torch.save({\n",
    "        'state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        }, os.path.join(checkpoint_dir, str(iter_number).zfill(6) + '.pth'))\n",
    "\n",
    "def load_checkpoint(pth_path, model, optimizer = None):\n",
    "    checkpoint = torch.load(pth_path)\n",
    "\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "    if not optimizer is None:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5.1: Implement the Residual Block (5 points)\n",
    "\n",
    "The authors of [5] explain the details about the proposed neural network in [6]. Start with implementing the residual block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        \n",
    "        # TODO Task 5.1\n",
    "        self.conv1 = ConvolutionalBlock(channels, channels, kernel_size=3, stride=1)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.conv2 = ConvolutionalBlock(channels, channels, kernel_size=3, stride=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # TODO Task 5.1\n",
    "        residual = x\n",
    "        out = self.relu(self.conv1(x))\n",
    "        out = self.conv2(out)\n",
    "        out = out + residual\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5.2: Implement the Convolutional Block (5 points)\n",
    "\n",
    "Refer to [5] and [6] for details. Note that the convolutional block shown in [6] in Figure 1 is only for explanatory reasons. Here the goal is to implement a convolutional block consisting of one convolutional layer, batch normalization and activation layer. However, feel free to adapt the structure, e.g. also to implement a convolutional block consisting of two conv layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, norm=\"batchNorm\"):\n",
    "        super(ConvolutionalBlock, self).__init__()\n",
    "        \n",
    "        # TODO Task 5.2\n",
    "        reflection_padding = kernel_size // 2\n",
    "        self.reflection_pad = torch.nn.ReflectionPad2d(reflection_padding)\n",
    "        self.conv2d = torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n",
    "        self.batchNorm = torch.nn.BatchNorm2d(out_channels, affine=True)\n",
    "        self.norm = norm\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # TODO Task 5.2\n",
    "        x = self.reflection_pad(x)\n",
    "        x = self.conv2d(x)\n",
    "        \n",
    "        if (self.norm==\"None\"):\n",
    "            out = x\n",
    "        else:\n",
    "            out = self.batchNorm(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5.3: Implement the Deconvolutional Block (5 points)\n",
    "\n",
    "In [6], the authors refer to the deconvolution as a convolution with a stride of 1/2. In PyTorch, this is achieved using a `nn.ConvTranspose2d`  layer. Be careful when setting values for `padding` and `output_padding`, such that the output size matches the size explained in [6]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeconvolutionalBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, output_padding):\n",
    "        super(DeconvolutionalBlock, self).__init__()\n",
    "        \n",
    "        # TODO Task 5.3\n",
    "       \n",
    "        padding_size = kernel_size // 2\n",
    "        self.conv_transpose = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding_size, output_padding)\n",
    "        self.batchNorm = nn.BatchNorm2d(out_channels, affine=True)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # TODO Task 5.3\n",
    "        x = self.conv_transpose(x)\n",
    "        x = self.batchNorm(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: Put it all together (5 points)\n",
    "\n",
    "Using the blocks from Tasks 5.1 to 5.3, assemble a neural network structure for style transfer (so not the one for super resolution) as explained in [5] and [6]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d6b531c2aba6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mStyleTransferNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mStyleTransferNetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;31m# TODO Task 5.4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class StyleTransferNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StyleTransferNetwork, self).__init__()\n",
    "        \n",
    "        # TODO Task 5.4\n",
    "        # Initial convolution layers\n",
    "        self.ConvBlock = nn.Sequential(\n",
    "            ConvolutionalBlock(3, 32, 9, 1),\n",
    "            nn.ReLU(),\n",
    "            ConvolutionalBlock(32, 64, 3, 2),\n",
    "            nn.ReLU(),\n",
    "            ConvolutionalBlock(64, 128, 3, 2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # Residual layers\n",
    "        self.ResBlock = nn.Sequential(\n",
    "            ResidualBlock(128), \n",
    "            ResidualBlock(128), \n",
    "            ResidualBlock(128), \n",
    "            ResidualBlock(128), \n",
    "            ResidualBlock(128)\n",
    "        )\n",
    "        \n",
    "        # Deconvolutional Layers \n",
    "        self.DeconvBlock = nn.Sequential(\n",
    "            DeconvolutionalBlock(128, 64, 3, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            DeconvolutionalBlock(64, 32, 3, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            ConvolutionalBlock(32, 3, 9, 1, norm=\"None\")\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # TODO Task 5.4\n",
    "        x = self.ConvBlock(x)\n",
    "        x = self.ResBlock(x)\n",
    "        x = self.DeconvBlock(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 1.0e-3\n",
    "batch_size = 4\n",
    "num_epochs = 2\n",
    "\n",
    "content_weight = 5e0\n",
    "style_weight = 1e2\n",
    "tv_weight = 1e-6\n",
    "\n",
    "plot_every = 1000 # 1000\n",
    "checkpoint_every = 1000# 5000\n",
    "\n",
    "content_img_name = 'chicago'\n",
    "content_img_path = os.path.join('img', 'content', content_img_name + '.jpg')\n",
    "content_torch = load_image(content_img_path).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen Style: mosaic\n"
     ]
    }
   ],
   "source": [
    "# TODO: Choose a style that you like and that you would like to train your model on by setting style_idx appropriately\n",
    "\n",
    "style_names = ['candy', 'composition_vii', 'feathers', 'la_muse', 'mosaic', 'starry_night_crop', 'the_scream' 'udnie', 'wave_crop']\n",
    "style_idx = 4\n",
    "\n",
    "print('Chosen Style:', style_names[style_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similiar to the authors of [5], we want to use vgg16 this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load VGG 16\n",
    "# get the \"features\" portion of VGG16 (we will not need the \"classifier\" portion)\n",
    "vgg16 = models.vgg16(pretrained = True).features.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): ReLU(inplace=True)\n",
      "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (3): ReLU(inplace=True)\n",
      "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (6): ReLU(inplace=True)\n",
      "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (8): ReLU(inplace=True)\n",
      "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (11): ReLU(inplace=True)\n",
      "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (13): ReLU(inplace=True)\n",
      "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (15): ReLU(inplace=True)\n",
      "  (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (18): ReLU(inplace=True)\n",
      "  (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (20): ReLU(inplace=True)\n",
      "  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (22): ReLU(inplace=True)\n",
      "  (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (25): ReLU(inplace=True)\n",
      "  (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (27): ReLU(inplace=True)\n",
      "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (29): ReLU(inplace=True)\n",
      "  (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(vgg16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6.1 Complete the training method, refer to [5] and [6] for details (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(style_name):\n",
    "\n",
    "    coco_dir = 'MSCOCO_256x256'\n",
    "\n",
    "    train_dataset = ImageDataset(root = os.path.join(coco_dir, 'train2014'))\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size = 4,\n",
    "        num_workers = 0,\n",
    "        shuffle = True\n",
    "    )\n",
    "\n",
    "    model = StyleTransferNetwork()\n",
    "    #if(torch.cuda.device_count() > 1):\n",
    "    #    print(\"Let's use\", torch.cuda.device_count(), \"GPUs\")\n",
    "    #   model = nn.DataParallel(model)\n",
    "    model.to(device)\n",
    "\n",
    "    #pth_path = os.path.join('checkpoints', '00000_checkpoint_mosaic_content_weight_5.0_style_weight_100.0_tv_weight_1e-06', '004000.pth')\n",
    "    #load_checkpoint(pth_path, model)\n",
    "\n",
    "\n",
    "    learning_curve = []\n",
    "\n",
    "    # freeze all VGG parameters since we're only optimizing the target image\n",
    "    for param in vgg16.parameters():\n",
    "        param.requires_grad_(False)\n",
    "        \n",
    "    # TODO Task 6.1: Similar as it was done before, find the correspondence of all image numbers and image names in vgg16 that are used in [5] for style transfer. You can again use the print(vgg16) command.\n",
    "    layers_vgg = {'3':'relu1_2',\n",
    "                  '8':'relu2_2',\n",
    "                  '15':'relu3_3',\n",
    "                  '22':'relu4_3'}\n",
    "\n",
    "    num_args = len(sys.argv)\n",
    "\n",
    "    style_image_path = os.path.join('img', 'style', style_name + '.jpg')\n",
    "    style_image = load_image(style_image_path).to(device)\n",
    "\n",
    "    style_features = get_features(style_image, vgg16, layers_vgg)\n",
    "    style_grams = {layer: gram_matrix(style_features[layer], normalize = True) for layer in style_features}\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "    \n",
    "    # TODO Task 6.1: Find the name of the content layer in vgg16 that is used in [5] for style transfer\n",
    "    content_layer = 'relu2_2'\n",
    "\n",
    "    # TODO Task 6.1: Find the names of the style layers in vgg16 that is used in [5] for style transfer and create a dictionary with entries 'name': 1.0\n",
    "    style_weights = {'relu1_2': 1.0,\n",
    "                     'relu2_2':1.0,\n",
    "                     'relu3_3':1.0,\n",
    "                     'relu4_3':1.0}\n",
    "\n",
    "    num_batches = len(train_loader)\n",
    "    num_iters = num_epochs * num_batches\n",
    "\n",
    "    num_iter = 0\n",
    "\n",
    "    weight_string = '_content_weight_{}_style_weight_{}_tv_weight_{}'.format(content_weight, style_weight, tv_weight)\n",
    "    run_id, checkpoint_dir = get_checkpoint_dir(style_names[style_idx] + weight_string)\n",
    "    \n",
    "    #print(\"Writing checkpoints to\", checkpoint_dir)\n",
    "\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "\n",
    "    running_loss = 0\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        for batch_idx, images in enumerate(train_loader):\n",
    "\t\t\t# Free-up unneeded cuda memory\n",
    "            #gc.collect()\n",
    "            #torch.cuda.empty_cache()\n",
    "\n",
    "            b, c, h, w = images.shape\n",
    "\n",
    "            images = images.to(device)\n",
    "            \n",
    "            # TODO Task 6.1: Get the features for the current batch of images\n",
    "            #content_features = None    ###Do not used [] outside for style_features\n",
    "            content_features = get_features(images, vgg16, layers_vgg)\n",
    "            \n",
    "            # TODO Task 6.1: Send the batch of images through the network\n",
    "            out = None\n",
    "            #out = vgg16(images)\n",
    "            out = model(images)\n",
    "\n",
    "            # TODO Task 6.1: Get the features for the model output\n",
    "            target_features = None\n",
    "            #target_features = get_features(images, vgg16,layers_vgg)\n",
    "            target_features = get_features(images.clone().requires_grad_(True).to(device), vgg16,layers_vgg)\n",
    "        \n",
    "            target_grams = {layer: gram_matrix(target_features[layer], normalize=True) for layer in target_features}\n",
    "\n",
    "            # Content Loss (Feature Loss)\n",
    "            content_loss = torch.mean((target_features[content_layer] - content_features[content_layer]) ** 2)\n",
    "\n",
    "            # Style Loss\n",
    "            style_loss = 0\n",
    "\n",
    "            # then add to it for each layer's gram matrix loss\n",
    "            for layer in style_weights:\n",
    "                target_gram = target_grams[layer]\n",
    "                style_gram = style_grams[layer]\n",
    "                layer_style_loss= style_weights[layer] *torch.mean((target_gram - style_gram.repeat(b,b)) ** 2)/b\n",
    "                style_loss += layer_style_loss / (h * w)**2\n",
    "                \n",
    "            ## TODO Task 6.1: Compute the anisotropic Total Variation loss of out according to https://en.wikipedia.org/wiki/Total_variation_denoising\n",
    "            tv_loss = 0\n",
    "            tv_loss = torch.sum(torch.abs(out[:, :, :, :-1] - out[:, :, :, 1:]))+\\\n",
    "            torch.sum(torch.abs(out[:, :, :-1, :] - out[:, :, 1:, :]))\n",
    "\n",
    "\n",
    "            # TODO Task 6.1: Compute the total weighted loss consisting of content loss, style loss and total variation loss\n",
    "            total_loss =  content_weight * content_loss + style_weight * style_loss + tv_weight * tv_loss\n",
    "\n",
    "            #running_loss += total_loss\n",
    "            running_loss += total_loss\n",
    "\n",
    "            # update weights\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            num_iter += 1\n",
    "\n",
    "            if num_iter % plot_every == 0:\n",
    "                model.eval()\n",
    "                print('%s (%d %d%%) %.4f' % (timeSince(start, num_iter / num_iters),\n",
    "                                                    num_iter, num_iter / num_iters * 100, running_loss / plot_every))\n",
    "                \n",
    "                learning_curve.append(running_loss / plot_every)\n",
    "\n",
    "                # Send current state image to Tensorboard\n",
    "               # out = model(content_torch)\n",
    "\n",
    "                #plt.imshow(im_convert(out))\n",
    "                #plt.show()\n",
    "\n",
    "                running_loss = 0\n",
    "\n",
    "                model.train()\n",
    "\n",
    "            if num_iter % checkpoint_every == 0:\n",
    "                # Write Checkpoint\n",
    "                print(\"Writing checkpoints to\", checkpoint_dir)\n",
    "                write_checkpoint(checkpoint_dir, model, optimizer, num_iter)\n",
    "                \n",
    "    return model, np.asarray(learning_curve)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_dir = 'MSCOCO_256x256'\n",
    "\n",
    "train_dataset = ImageDataset(root = os.path.join(coco_dir, 'train2014'))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size = 4,\n",
    "        num_workers = 0,\n",
    "        shuffle = True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 391, 470])\n",
      "torch.Size([256, 256])\n",
      "torch.Size([4, 3, 256, 256])\n",
      "torch.Size([4, 256, 64, 64])\n",
      "torch.Size([256, 256])\n"
     ]
    }
   ],
   "source": [
    "layers_vgg16 = {'3':'relu1_2',\n",
    "                  '8':'relu2_2',\n",
    "                  '15':'relu3_3',\n",
    "                  '22':'relu4_3'}\n",
    "style_image_path = os.path.join('img', 'style', style_names[style_idx] + '.jpg')\n",
    "style_image = load_image(style_image_path).to(device)\n",
    "#style_image = style_image.repeat(4,1,1,1)\n",
    "print(style_image.shape)\n",
    "\n",
    "style_features = get_features(style_image, vgg16, layers_vgg16)\n",
    "style_grams = {layer: gram_matrix(style_features[layer], normalize = True) for layer in style_features}\n",
    "print(style_grams['relu1_2'].shape)\n",
    "model = StyleTransferNetwork().to(device)\n",
    "for batch_idx, images in enumerate(train_loader):\n",
    "    mytarget = images.to(device)\n",
    "    print(mytarget.shape)\n",
    "    target_features =  get_features(mytarget, vgg16,layers_vgg16)\n",
    "    print(target_features['relu3_3'].shape)\n",
    "    target_grams = {layer: gram_matrix(target_features[layer], normalize=True) for layer in target_features}\n",
    "    print(target_grams['relu1_2'].shape)\n",
    "    style_gram = style_grams['relu1_2']\n",
    "    \n",
    "    target_gram = target_grams['relu1_2']\n",
    "    torch.add(target_gram, style_gram)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1m 17s (- 51m 59s) (1000 2%) 748.2557\n",
      "Writing checkpoints to checkpoints/00012_checkpoint_mosaic_content_weight_5.0_style_weight_100.0_tv_weight_1e-06\n",
      "2m 34s (- 50m 48s) (2000 4%) 748.6340\n",
      "Writing checkpoints to checkpoints/00012_checkpoint_mosaic_content_weight_5.0_style_weight_100.0_tv_weight_1e-06\n",
      "3m 52s (- 49m 36s) (3000 7%) 748.4108\n",
      "Writing checkpoints to checkpoints/00012_checkpoint_mosaic_content_weight_5.0_style_weight_100.0_tv_weight_1e-06\n",
      "5m 10s (- 48m 18s) (4000 9%) 749.7643\n",
      "Writing checkpoints to checkpoints/00012_checkpoint_mosaic_content_weight_5.0_style_weight_100.0_tv_weight_1e-06\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 64.00 MiB (GPU 0; 31.75 GiB total capacity; 27.35 GiB already allocated; 47.75 MiB free; 3.31 GiB cached)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-4f7965d8f779>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# TODO: Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_curvce\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstyle_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstyle_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-20-f49e7a76e3ee>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(style_name)\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0;31m# update weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0mtotal_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ML/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    148\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \"\"\"\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ML/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 0; 31.75 GiB total capacity; 27.35 GiB already allocated; 47.75 MiB free; 3.31 GiB cached)"
     ]
    }
   ],
   "source": [
    "# TODO: Train the model\n",
    "model, learning_curvce = train_model(style_names[style_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
